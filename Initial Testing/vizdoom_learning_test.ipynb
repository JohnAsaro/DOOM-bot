{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependancies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import * #Import all of vizdoom\n",
    "import numpy as np #Numpy for identity matrix\n",
    "import time #To make the program sleep (wait), so we can actually see what's happening\n",
    "from stable_baselines3.common import env_checker #Import the env_checker class from stable_baselines3 to check the environment\n",
    "from stable_baselines3 import PPO #Import the PPO class for training\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #Import the evaluate_policy function to evaluate the model\n",
    "import os #To save the model to the correct path\n",
    "from vizdoom_with_ai_gym_env_test import VizDoomGym_Simple, Deadly_Corridor_VZG, TrainAndLogCallback #Import the environment class and TrainAndLogCallback \n",
    "from pathfinder import doomfinder, create_new_checkpoint_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = create_new_checkpoint_directory('best_model_PPO_test_basic') #Directory to save the model\n",
    "LOG_DIR = './logs/log_PPO_test_basic' #Directory to save the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLogCallback(check_freq=20000, save_path=CHECKPOINT_DIR) #After every 20000 steps of training model, we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym_Simple(config_path=doomfinder(\"basic.cfg\"), render=False) #Create the environment\n",
    "print(env.get_state().game_variables) #Print the game variables\n",
    "env_checker.check_env(env) #Check the environment to see if its valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render(render_in_greyscale=True) #Render the environment in greyscale, crashes the whole thing now and IDK why, not particularly important to fix ATM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PPO algorithm for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Env already created in previous cell\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0001, n_steps=2048) #Create the model\n",
    "\n",
    "#CnnPolicy is a convolutional neural network policy, which is used for images\n",
    "#env is the environment\n",
    "#verbose is the verbosity level\n",
    "#tensorboard_log is the directory to save the logs\n",
    "#learning_rate is the learning rate of the model\n",
    "#n_steps is the number of steps to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.learn(total_timesteps=100000, callback=callback) #Train the model for 100000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PPO.load('./Training/checkpoints/best_model_PPO_test_basic_2/best_model_50000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_basic_3/best_model_100000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym_Simple(config_path=doomfinder('basic.cfg'), render=True) #Reload env with rendering enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100, render=True) #Evaluate the model for 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for 5 episodes but sleep so that we can see whats going on\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  # Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly the model performs much worse when trained for 1M steps than when trained for 100k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_basic_1/best_model_100000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for 5 episodes but sleep so that we can see whats going on\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  # Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a new level (Defend The Center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym_Simple(config_path=doomfinder('defend_the_center.cfg'), render=True) #Reload env with new map\n",
    "print(env.get_state().game_variables) #Print the game variables\n",
    "env_checker.check_env(env) #Check the environment to see if its valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try with old model (boooo its not good at this map)\n",
    "\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_basic_4/best_model_50000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  # Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = create_new_checkpoint_directory('best_model_PPO_test_defend_the_center') #Directory to save the model\n",
    "LOG_DIR = './logs/log_PPO_test_defend_the_center' #Directory to save the logs\n",
    "callback = TrainAndLogCallback(check_freq=25000, save_path=CHECKPOINT_DIR) #After every 25000 steps of training model, we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym_Simple(config_path=doomfinder('defend_the_center.cfg'), render=False) #Reload env with new map\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0001, n_steps=8192) #Create the model with more n_steps, more n_steps for more complex things\n",
    "model.learn(total_timesteps=300000, callback=callback) #Train the model for 300000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_defend_the_center_14/best_model_500000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = VizDoomGym_Simple(config_path=doomfinder('defend_the_center.cfg'), render=True) #Reload env with rendering enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10, render=True) #Evaluate the model for 10 episodes\n",
    "print(mean_reward) #Print the mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 2.1399999999999615\n",
      "Episode: 1, Total Reward: -15.180000000000007\n",
      "Episode: 2, Total Reward: -5.320000000000005\n",
      "Episode: 3, Total Reward: -4.660000000000046\n",
      "Episode: 4, Total Reward: 8.39\n"
     ]
    }
   ],
   "source": [
    "#Try with new model\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  #Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see if it works, lets try and do the previous model test but now let it move in all directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env #Import OpenAI Gym's Env class\n",
    "from gymnasium.spaces import Discrete, Box #Import OpenAI Gym's Discrete and Box spaces\n",
    "import cv2 #OpenCV for image processing, used for modifying the DOOM environment to make it run faster \n",
    "\n",
    "class VizDoomGym_Simple_All_Dir(Env): #Copy of last environment class, but now we can move in all directions\n",
    "    def __init__(self, config_path, render=False): #Constructor\n",
    "        \n",
    "        #Configs this is used for: basic.cfg, defend_the_center.cfg\n",
    "\n",
    "        super(VizDoomGym_Simple_All_Dir, self).__init__() #Inherit from Env class\n",
    "\n",
    "        #Args: \n",
    "            #config_path (str): The path to the configuration file\n",
    "            #render (bool): Whether to render the environment or not, false by default\n",
    "\n",
    "        #Setup game\n",
    "        self.game = vizdoom.DoomGame() #Create a DoomGame object\n",
    "        self.game.load_config(config_path) #Load the configuration file from file path, ex: doomfinder(\"basic.cfg\")\n",
    "\n",
    "        #Set window visibility\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        self.game.init() #Start the game\n",
    "\n",
    "        #Setup action and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8) #Observation space, 100x160x1 image\n",
    "        self.action_space = Discrete(7) #Action space, 7 actions\n",
    "\n",
    "        #Game variables\n",
    "        self.ammo = self.game.get_state().game_variables[0]  #Get the ammo count, initialize to the current ammo\n",
    "        self.health = 100 #Initialize health to 100 (assuming we start at full health)\n",
    "\n",
    "\n",
    "    def step(self, action, limit = 1000): #Take a step in the environment \n",
    "        #Args:\n",
    "            #action (int): The action to take\n",
    "            #limit (int): Unimplemented \"limit\" for the episode, most likely will be a time limit\n",
    "        #Returns:\n",
    "            #observation (np.array): The screen buffer of the environment\n",
    "            #reward (float): The reward for the action taken\n",
    "            #terminated (bool) Whether the episode is finished or not (by reaching the goal)\n",
    "            #truncated (bool): Whether the episode has reached some terminal state without reaching the goal (ie: running out of time)\n",
    "            #info (dict): Additional information about the environment\n",
    "\n",
    "        #Specify actions and take a step\n",
    "        actions = np.identity(7) #Create an identity matrix with 7 rows (7 actions), TURN_LEFT, TURN_RIGHT, MOVE_FOWARD, MOVE_BACKWARDS, ATTACK, MOVE_LEFT, MOVE_RIGHT,  these are the actions we can take in the environment\n",
    "        movement_reward = self.game.make_action(actions[action], 4) #Reward for taking a random action, second parameter is frame skip (skip 4 frames before taking the next action), the reason we do this is because it saves us time while being easy to see what is happening \n",
    "        reward = movement_reward #Initialize reward to movement reward\n",
    "        truncated = False #Not implemented yet, so set to False. The idea is that if step passes some sort of limit, like a time limit, then the episode is truncated.\n",
    "        info = {} #Initialize info to an empty dictionary\n",
    "        Basic = False\n",
    "\n",
    "        if self.game.get_state(): #If the game is not finished\n",
    "            observation = self.game.get_state().screen_buffer #Get the screen buffer\n",
    "            observation = self.greyscale(observation) #Convert the image to greyscale\n",
    "\n",
    "            #Get game variables\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            if len(game_variables) == 2:\n",
    "                ammo, health = game_variables\n",
    "            else:\n",
    "                ammo = game_variables[0]\n",
    "                health = 100 #Assume health is 100 if it's not provided (effectively ignoring it)\n",
    "                Basic = True\n",
    "            \n",
    "            #Calculate reward deltas\n",
    "            if(Basic == False): #If its the basic config we just ignore all deltas entirely, I know this is janky but its just a testing enviorment so whatever\n",
    "                ammo_delta = ammo - self.ammo #Current ammo - old ammo = ammo used\n",
    "                ammo = self.ammo \n",
    "                health_delta = health - self.health  #Current health - old health = damage taken\n",
    "                health = self.health\n",
    "            \n",
    "                #reward = movement_reward*2 + ammo_delta*0.0384615385 + health_delta*0.01 #Calculate the reward, the idea is the max score is 2, if we lose all heath our score is subtracted by 1, if we lose all ammo our score is subtracted by 1\n",
    "                reward = movement_reward*2 + ammo_delta*0.01 + health_delta*0.00 #Calculate the reward, the idea is the max score is 2, if we lose all heath our score is subtracted by 1, if we lose all ammo our score is subtracted by 1\n",
    "\n",
    "            info = {\"ammo\": ammo}\n",
    "        else:\n",
    "            observation = np.zeros(self.observation_space.shape) #Return a blank screen\n",
    "\n",
    "        terminated = self.game.is_episode_finished() #Check if the episode is finished\n",
    "\n",
    "        return observation, movement_reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, render_in_greyscale=False): #Render the environment for a frame\n",
    "        #Args:\n",
    "            #render_in_greyscale (bool): Whether to render the environment in greyscale or not\n",
    "        \n",
    "        if self.game.get_state() and render_in_greyscale:  #Only render if there's a valid game state\n",
    "            observation = self.game.get_state().screen_buffer\n",
    "            greyscale_obs = self.greyscale(observation)  #Convert to greyscale\n",
    "            #Render using OpenCV to visualize\n",
    "            cv2.imshow(\"VizDoom Environment\", greyscale_obs.squeeze())  #Remove extra dimension and display\n",
    "            cv2.waitKey(1)  #Wait 1ms between frames to allow for rendering\n",
    "        elif self.game.get_state():  #Only render if there's a valid game state\n",
    "            observation = self.game.get_state().screen_buffer\n",
    "            #Render using OpenCV to visualize\n",
    "            cv2.imshow(\"VizDoom Environment\", observation.squeeze())  #Remove extra dimension and display\n",
    "            cv2.waitKey(1)  #Wait 1ms between frames to allow for rendering\n",
    "        else:\n",
    "            print(\"No game state to render.\")\n",
    "\n",
    "            \n",
    "    def reset(self, seed=None): #Reset the environment when we start a new game\n",
    "        #Args:\n",
    "            #seed (int): The seed for the random number generator\n",
    "        #Returns:\n",
    "            #(observation, info) (tuple)\n",
    "                #observation (np.array): The screen buffer of the environment\n",
    "                #info (dict): Additional information about the environment\n",
    "            \n",
    "        super().reset(seed=seed) #Implement seeding\n",
    "        \n",
    "        self.game.new_episode() #Start a new episode\n",
    "        state = self.game.get_state().screen_buffer #Get the screen buffer\n",
    "        observation = self.greyscale(state) #Convert the image to greyscale\n",
    "        \n",
    "        #Gather any additional environment-specific info (like ammo, etc.)\n",
    "        if self.game.get_state():\n",
    "            ammo = self.game.get_state().game_variables[0]  #Get the ammo count\n",
    "            info = {\"ammo\": ammo}\n",
    "        else:\n",
    "            info = {} #No gamestate means no info can be gathered\n",
    "        \n",
    "        return (observation, info) #Tuple of observation and info\n",
    "\n",
    "    def greyscale(self, observation=None): #Convert the enivornment to greyscale and resize it\n",
    "        #Args:\n",
    "            #observation (np.array): The image of the environment (the current game frame)\n",
    "        #Returns:\n",
    "            #grey_return (np.array): The resized greyscale image of the environment\n",
    "        \n",
    "        if observation is None and self.game.get_state(): #If no observation is passed\n",
    "            observation = self.game.get_state().screen_buffer #Get the screen buffer \n",
    "\n",
    "        grey = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) #Convert the image to greyscale\n",
    "        resize = cv2.resize(grey, (160, 100), interpolation=cv2.INTER_CUBIC) #Resize the image to 160x100\n",
    "        state = np.reshape(resize, (100, 160, 1)) #Reshape the image to 100x160x1\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def get_state(self): \n",
    "        #Returns:\n",
    "            #state (np.array): The current state of the environment\n",
    "        return self.game.get_state()\n",
    "\n",
    "    def close(self): #Close the environment\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = create_new_checkpoint_directory('best_model_PPO_test_defend_the_center') #Directory to save the model\n",
    "LOG_DIR = './logs/log_PPO_test_defend_the_center' #Directory to save the logs\n",
    "callback = TrainAndLogCallback(check_freq=25000, save_path=CHECKPOINT_DIR) #After every 25000 steps of training model, we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym_Simple_All_Dir(config_path=doomfinder('defend_the_center_all_directions.cfg'), render=False) #Reload env with new map\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0001, n_steps=8192) #Create the model with more n_steps, more n_steps for more complex things\n",
    "model.learn(total_timesteps=1000000, callback=callback) #Train the model for 1000000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_defend_the_center_16/best_model_1000000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = VizDoomGym_Simple_All_Dir(config_path=doomfinder('defend_the_center_all_directions.cfg'), render=True) #Reload env with rendering enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 26.0\n",
      "Episode: 1, Total Reward: 8.0\n",
      "Episode: 2, Total Reward: 21.0\n",
      "Episode: 3, Total Reward: 9.0\n",
      "Episode: 4, Total Reward: 23.0\n"
     ]
    }
   ],
   "source": [
    "#Try with new model\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  #Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New test, start with the model we trained in a less complicated env, run the tests again starting at that end point, with the reward shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = create_new_checkpoint_directory('best_model_PPO_test_defend_the_center') #Directory to save the model\n",
    "LOG_DIR = './logs/log_PPO_test_defend_the_center' #Directory to save the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_defend_the_center_2/best_model_100000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = (VizDoomGym_Simple(config_path=doomfinder('defend_the_center.cfg'), render=False)) #Reload env with new map \n",
    "model.set_env(env) #Set the environment for the model\n",
    "callback = TrainAndLogCallback(check_freq=25000, save_path=CHECKPOINT_DIR) #After every 25000 steps of training model, we save the model\n",
    "model.learn(total_timesteps=1000000, callback=callback) #Train the model for 1000000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing but with all dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CHECKPOINT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m (VizDoomGym_Simple_All_Dir(config_path\u001b[38;5;241m=\u001b[39mdoomfinder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefend_the_center_all_directions.cfg\u001b[39m\u001b[38;5;124m'\u001b[39m), render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;66;03m#Reload env with new map \u001b[39;00m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env) \u001b[38;5;66;03m#Set the environment for the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m callback \u001b[38;5;241m=\u001b[39m TrainAndLogCallback(check_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25000\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[43mCHECKPOINT_DIR\u001b[49m) \u001b[38;5;66;03m#After every 25000 steps of training model, we save the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcallback) \u001b[38;5;66;03m#Train the model for 1000000 steps\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CHECKPOINT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_defend_the_center_16/best_model_1000000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = (VizDoomGym_Simple_All_Dir(config_path=doomfinder('defend_the_center_all_directions.cfg'), render=False)) #Reload env with new map \n",
    "model.set_env(env) #Set the environment for the model\n",
    "callback = TrainAndLogCallback(check_freq=25000, save_path=CHECKPOINT_DIR) #After every 25000 steps of training model, we save the model\n",
    "model.learn(total_timesteps=1000000, callback=callback) #Train the model for 1000000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with more complicated config (needs custom env, and moving foward each config more complicated than this will probably also need their own env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s1.cfg'), render=False) #Reload env with new map, easy version\n",
    "env_checker.check_env(env) #Check the environment to see if its valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = create_new_checkpoint_directory('best_model_PPO_test_deadly_corridor') #Directory to save the model\n",
    "LOG_DIR = './logs/log_PPO_test_deadly_corridor' #Directory to save the logs\n",
    "callback = TrainAndLogCallback(check_freq=25000, save_path=CHECKPOINT_DIR) #After every 25000 steps of training model, we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9) #Create the model with more complicated hyperparameters because of the more complicated enviornment\n",
    "#Changes: Learning rate 0.0001 -> 0.00001, n_steps 4096 -> 8192, added clip range, gamma andd gae_lambda\n",
    "model.learn(total_timesteps=400000, callback=callback) #Train the model for 400000 steps\n",
    "\n",
    "#Load the model that was created training on the easy difficulty\n",
    "\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_deadly_corridor_1/best_model_400000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s2.cfg'), render=False) #Reload env with harder version of map\n",
    "model.set_env(env) #Set the env to the model\n",
    "model.learn(total_timesteps=50000, callback=callback) #Train the model for 50000 steps on this higher difficulty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the process up to the hardest difficulty\n",
    "\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_deadly_corridor_1/best_model_450000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s3.cfg'), render=False) #Reload env with harder version of map\n",
    "model.set_env(env) #Set the env to the model\n",
    "model.learn(total_timesteps=50000, callback=callback) #Train the model for 50000 steps on this higher difficulty\n",
    "\n",
    "\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_deadly_corridor_1/best_model_500000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s4.cfg'), render=False) #Reload env with harder version of map\n",
    "model.set_env(env) #Set the env to the model\n",
    "model.learn(total_timesteps=50000, callback=callback) #Train the model for 50000 steps on this higher difficulty\n",
    "\n",
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_deadly_corridor_1/best_model_550000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s5.cfg'), render=False) #Reload env with harder version of map\n",
    "model.set_env(env) #Set the env to the model\n",
    "model.learn(total_timesteps=50000, callback=callback) #Train the model for 50000 steps on this higher difficulty\n",
    "\n",
    "#This should allow the model some time to adjust to the higher difficulties, and should allow it to learn the map better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./Training/checkpoints/best_model_PPO_test_deadly_corridor_1/best_model_625000.zip') #Load the model (hardcoded to load a specific model but adjust as needed)\n",
    "env = Deadly_Corridor_VZG(config_path=doomfinder('deadly_corridor_s5.cfg'), render=True) #Reload env with rendering enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -472.9422607421875\n",
      "Episode: 1, Total Reward: -472.9422607421875\n",
      "Episode: 2, Total Reward: -319.30694580078125\n",
      "Episode: 3, Total Reward: -472.9422607421875\n",
      "Episode: 4, Total Reward: -472.9422607421875\n"
     ]
    }
   ],
   "source": [
    "#Try with new model\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "    while not done:  #While the game isn't done\n",
    "        action, _ = model.predict(obs)  #Get the action\n",
    "        obs, reward, done, truncated, info = env.step(action)  #Take the action\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "    print('Episode: {}, Total Reward: {}'.format(episode, total_reward))  #Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
