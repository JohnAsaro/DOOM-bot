{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a ACO to learn health gathering and mywayhome (not done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from vizdoom import * #Import all of vizdoom\n",
    "import time #To make the program sleep (wait), so we can actually see what's happening\n",
    "from gymnasium import Env #Import OpenAI Gym's Env class\n",
    "from gymnasium.spaces import Discrete, Box #Import OpenAI Gym's Discrete and Box spaces\n",
    "import cv2 #OpenCV for image processing, used for modifying the DOOM environment to make it run faster \n",
    "from stable_baselines3.common.callbacks import BaseCallback #Import the BaseCallback class from stable_baselines3 to learn from the environment\n",
    "from stable_baselines3.common import env_checker #Import the env_checker class from stable_baselines3 to check the environment\n",
    "import os #To create directories for saving models\n",
    "import sys #To change the path so we can import the pathfinder module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "original_sys_path = sys.path.copy() #Come back to this path later after we navigate to the parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  #Add the parent directory to the path so we can import the pathfinder module\n",
    "from pathfinder import doomfinder, create_new_best_generation_directory, gamefinder #Import functions from the pathfinder module\n",
    "sys.path = original_sys_path #Set the path back to the original path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define enviornment (for mywayhome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note we are using are representing the map as a 960x832 grid\n",
    "\n",
    "#Map extreme verticies\n",
    "MIN_X, MAX_X = 160, 1120\n",
    "MIN_Y, MAX_Y = -704, 128\n",
    "\n",
    "#Grid dimensions, the map size is actually 960x832 but this allows us some leeway if we get close to the edge\n",
    "GRID_WIDTH = 960 #1 unit per cell\n",
    "GRID_HEIGHT = 832 #1 units per cell\n",
    "\n",
    "class mywayhome_VZG(Env): #Used for mywayhome config\n",
    "    def __init__(self, config_path, render=False, grid_size=(GRID_HEIGHT, GRID_WIDTH), pheromone_evaporation_rate=0.001, pheromone_map = None): #Constructor\n",
    "        #Args:\n",
    "            #config_path (str): The path to the configuration file\n",
    "            #render (bool): Whether to render the environment or not, false by default\n",
    "            #grid_size (tuple): The size of the grid (x, y) for the environment\n",
    "            #pheromone_evaporation_rate (float): The rate at which pheromones evaporate over time\n",
    "            #pheromone_map (np.array): Optional pre-filled pheromone map, if not provided, it will be initialized with low pheromone levels\n",
    "\n",
    "        super(mywayhome_VZG, self).__init__() #Inherit from Env class\n",
    "\n",
    "        #Setup game environment\n",
    "        self.game = vizdoom.DoomGame() #Create a DoomGame object\n",
    "        self.game.set_doom_game_path(gamefinder('DOOM2.WAD')) #Set the path to the game\n",
    "        self.game.load_config(config_path) #Load the configuration file from file path\n",
    "        self.game.set_window_visible(render) #Set window visibility based on render argument\n",
    "        self.game.init() #Initialize the game\n",
    "\n",
    "        #Setup action and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8) #Observation space, 100x160x1 image\n",
    "        self.action_space = Discrete(6) #Action space, 6 actions (turn left, turn right, move forward, move left, move right, move backwards)\n",
    "\n",
    "        #Grid dimensions and pheromone setup\n",
    "        self.grid_size = grid_size #Set the grid size as specified \n",
    "        if pheromone_map is not None:\n",
    "            self.pheromone_map = pheromone_map #Use the provided pheromone map if available\n",
    "        else: #If no pheromone map is provided, initialize a new one\n",
    "            self.pheromone_map = np.ones(grid_size) * 0  #Start with adjustable pheromone levels across the grid (adjustable by the multiplier)\n",
    "        self.pheromone_evaporation_rate = pheromone_evaporation_rate  #Controls how fast pheromones decay over time\n",
    "\n",
    "        #Get game variables\n",
    "        game_variables = self.game.get_state().game_variables\n",
    "        xpos, ypos, angle = game_variables #Unpack the game variables\n",
    "        \n",
    "        self.xpos = xpos #X position of the player\n",
    "        self.ypos = ypos #Y position of the player\n",
    "        self.angle = angle #Angle of the player \n",
    "\n",
    "    def coord_to_grid(self, x, y): #Convert coordinates to grid indices\n",
    "        #Args:\n",
    "            #x (int): X coordinate to convert\n",
    "            #y (int): Y coordinate to convert\n",
    "        #Returns:\n",
    "            #grid_x, grid_y (tuple): The grid indices corresponding to the coordinates\n",
    "        #Raises:\n",
    "            #ValueError: If the coordinates are out of bounds\n",
    "\n",
    "        #Ensure coordinates are within world bounds\n",
    "        if x < MIN_X or x > MAX_X or y < MIN_Y or y > MAX_Y:\n",
    "            raise ValueError(f\"Coordinates ({x}, {y}) are out of bounds!\")\n",
    "        \n",
    "        #Normalize coordinates to range [0, 1]\n",
    "        norm_x = (x - MIN_X) / (MAX_X - MIN_X)\n",
    "        norm_y = (y - MIN_Y) / (MAX_Y - MIN_Y)\n",
    "        \n",
    "        print(f\"norm_x = {norm_x}, norm_y = {norm_y}\") #Debug print\n",
    "\n",
    "        #Scale to grid dimensions and floor the values to ensure integers\n",
    "        grid_x = int(norm_x * (GRID_WIDTH - 1))\n",
    "        grid_y = int(norm_y * (GRID_HEIGHT - 1))\n",
    "\n",
    "        print(f\"grid_x = {grid_x}, grid_y = {grid_y}\") #Debug print\n",
    "\n",
    "        return (grid_x, grid_y)\n",
    "\n",
    "    def step(self, action, limit = 1000): #Take a step in the environment \n",
    "        #Args:\n",
    "            #action (int): The action to take\n",
    "            #limit (int): Unimplemented \"limit\" for the episode, most likely will be a time limit\n",
    "        #Returns:\n",
    "            #observation (np.array): The screen buffer of the environment\n",
    "            #reward (float): The reward for the action taken\n",
    "            #terminated (bool) Whether the episode is finished or not (by reaching the goal)\n",
    "            #truncated (bool): Whether the episode has reached some terminal state without reaching the goal (ie: running out of time)\n",
    "            #info (dict): Additional information about the environment\n",
    "\n",
    "        print(f\"xpos = {self.xpos}, ypos = {self.ypos}, angle = {self.angle}\") #Debug print\n",
    "\n",
    "        #Specify actions and take a step\n",
    "        actions = np.identity(6) #Create an identity matrix with 6 rows (6 actions), TURN_LEFT, TURN_RIGHT, MOVE_FOWARD, MOVE_LEFT, MOVE_RIGHT, MOVE_BACKWARD, these are the actions we can take in the environment\n",
    "        self.game.make_action(actions[action], 4) #Take an action in the action space, second parameter is frame skip (skip 4 frames before taking the next action), the reason we do this is because it saves us time while being easy to see what is happening \n",
    "        truncated = False #Not implemented yet, so set to False. The idea is that if step passes some sort of limit, like a time limit, then the episode is truncated.\n",
    "        info = {\"xpos\": self.xpos, \"ypos\": self.ypos, \"angle\": self.angle} # Initialize info with current position and angle\n",
    "        reward = 0 #Initialize reward to 0\n",
    "\n",
    "        if self.game.get_state(): #If the game is not finished\n",
    "            observation = self.game.get_state().screen_buffer #Get the screen buffer\n",
    "            observation = self.greyscale(observation) #Convert the image to greyscale\n",
    "            \n",
    "            (prev_grid_x, prev_grid_y) = self.coord_to_grid(self.xpos, self.ypos) #Find the grid indices of the previous position\n",
    "            xpos, ypos, angle = self.game.get_state().game_variables #Get the game variables\n",
    "            self.xpos = xpos #Update the x position\n",
    "            self.ypos = ypos #Update the y position\n",
    "            self.angle = angle #Update the angle\n",
    "\n",
    "            info = {\"xpos\": self.xpos, \"ypos\": self.ypos, \"angle\": self.angle} #Add position and angle to the info dictionary\n",
    "            (grid_x, grid_y) = self.coord_to_grid(xpos, ypos) #Find the grid indices of the current position\n",
    "\n",
    "        #Update pheromones based on movement feedback\n",
    "            movement_actions = [2, 3, 4, 5] #The actions that move the agent\n",
    "            if (grid_x, grid_y) == (prev_grid_x, prev_grid_y) and (action in movement_actions): #No movement detected\n",
    "                self.pheromone_map[grid_y, grid_x] = 0 #Mark as a wall\n",
    "            else: #If this isnt a wall, add pheromones\n",
    "                self.pheromone_map[grid_y, grid_x] += 0.1 #Deposit pheromones\n",
    "                self.pheromone_map *= (1 - self.pheromone_evaporation_rate) #Simulate evaporation\n",
    "        else: #If the game is finished\n",
    "            observation = np.zeros(self.observation_space.shape) #Return a blank screen\n",
    "\n",
    "        terminated = self.game.is_episode_finished() #Check if the episode is finished\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self): #Reset the environment when we start a new game\n",
    "        #Args:\n",
    "            #seed (int): The seed for the random number generator\n",
    "        #Returns:\n",
    "            #(observation, info) (tuple)\n",
    "                #observation (np.array): The screen buffer of the environment\n",
    "                #info (dict): Additional information about the environment\n",
    "\n",
    "        self.game.new_episode() #Start a new episode\n",
    "        game_variables = self.game.get_state().game_variables  #Get the game variables   \n",
    "        self.xpos, self.ypos, self.angle = game_variables  #Unpack the game variables, set agent to its random starting position \n",
    "\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.greyscale(state)  #Convert the initial screen state to greyscale\n",
    "        info = {\"xpos\": self.xpos, \"ypos\": self.ypos, \"angle\": self.angle} #Add position and angle to the info dictionary\n",
    "\n",
    "        return observation, info  #Return the observation and any additional info\n",
    "\n",
    "    def greyscale(self, observation): #Convert the environment image to greyscale and resize it\n",
    "        #Args:\n",
    "            #observation (np.array): The image of the environment (the current game frame)\n",
    "        #Returns:\n",
    "            #grey_return (np.array): The resized greyscale image of the environment\n",
    "        \n",
    "        grey = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) #Convert the image to greyscale\n",
    "        resize = cv2.resize(grey, (160, 100), interpolation=cv2.INTER_CUBIC) #Resize the image to 160x100 (frame size)\n",
    "        state = np.reshape(resize, (100, 160, 1)) #Reshape to 100x160x1 for compatibility with neural networks\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def render(self, render_in_greyscale=False): #Render the environment for a frame\n",
    "        #Args:\n",
    "            #render_in_greyscale (bool): Whether to render the environment in greyscale or not\n",
    "        \n",
    "        if self.game.get_state() and render_in_greyscale:  #Only render if there's a valid game state\n",
    "            observation = self.game.get_state().screen_buffer\n",
    "            greyscale_obs = self.greyscale(observation)  #Convert the observation to greyscale\n",
    "            cv2.imshow(\"VizDoom Environment\", greyscale_obs.squeeze())  #Remove extra dimension and display\n",
    "            cv2.waitKey(1)  #Wait 1ms between frames to allow for rendering\n",
    "        elif self.game.get_state():  #Only render if there's a valid game state\n",
    "            observation = self.game.get_state().screen_buffer\n",
    "            cv2.imshow(\"VizDoom Environment\", observation.squeeze())  #Render the environment without greyscale\n",
    "            cv2.waitKey(1)  #Wait 1ms between frames to allow for rendering\n",
    "        else:\n",
    "            print(\"No game state to render.\")\n",
    "\n",
    "    def get_state(self): #Get the current state of the environment\n",
    "        #Returns:\n",
    "            #state (np.array): The current state of the environment\n",
    "        return self.game.get_state()\n",
    "\n",
    "    def get_agent_position(self): #Get the current position of the agent (tuple of x, y coordinates)\n",
    "        #Returns:\n",
    "            #(xpos, ypos) (tuple): The current position of the agent\n",
    "\n",
    "        return (self.xpos, self.ypos)\n",
    "\n",
    "    def close(self): #Close the environment when done\n",
    "        self.game.close()  #Terminate the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training (I gotta update the goal to include all possible goal positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pheromone_grid(pheromone_grid, iteration):\n",
    "    plt.imshow(pheromone_grid, cmap='hot', interpolation='nearest')\n",
    "    plt.title(f'Pheromone Grid at Iteration {iteration}')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def aco_training(env, num_ants=10, iterations=50, alpha=5.0, beta=2.0, rho=0.05, pheromone_deposit=1.0):\n",
    "    \"\"\"\n",
    "    Train a bot to pathfind in the ViZDoom environment using ACO.\n",
    "    \n",
    "    Args:\n",
    "        env (Env): The ViZDoom environment.\n",
    "        num_ants (int): Number of ants (pathfinding agents) per iteration.\n",
    "        iterations (int): Total iterations for ACO.\n",
    "        alpha (float): Importance of pheromones.\n",
    "        beta (float): Importance of heuristic (distance to goal).\n",
    "        rho (float): Pheromone evaporation rate.\n",
    "        pheromone_deposit (float): Amount of pheromone deposited on successful paths.\n",
    "    \"\"\"\n",
    "    #Initialize pheromone grid\n",
    "    pheromone_grid = env.pheromone_map\n",
    "    \n",
    "    GOAL_POSITION = env.coord_to_grid(1040, -316)  #Armor is at 1040, -316\n",
    "    \n",
    "    #Heuristic function: inverse distance to goal\n",
    "    def heuristic(x, y):\n",
    "        gx, gy = GOAL_POSITION\n",
    "        return 1.0 / (np.sqrt((gx - x) ** 2 + (gy - y) ** 2) + 1e-6)\n",
    "\n",
    "    #Simulate movement locally without stepping the game, this is unfinished I have to account for angle\n",
    "    def simulate_move(x, y, angle, move):\n",
    "        print(f\"x passed is {x}, y passed is {y}, angle passed is {angle}\")  #Debug print\n",
    "\n",
    "        step_size = 1  #Approximate movement step size; tune based on game behavior\n",
    "        angle_rad = np.deg2rad(angle)  #Convert angle to radians\n",
    "\n",
    "        if move == 0:  #TURN_LEFT (no position change)\n",
    "            return x, y, angle - 90\n",
    "        elif move == 1:  #TURN_RIGHT (no position change)\n",
    "            return x, y, angle + 90\n",
    "        elif move == 2:  #MOVE_FORWARD\n",
    "            return x + step_size * np.cos(angle_rad), y + step_size * np.sin(angle_rad), angle\n",
    "        elif move == 3:  #MOVE_LEFT\n",
    "            return x - step_size * np.sin(angle_rad), y + step_size * np.cos(angle_rad), angle\n",
    "        elif move == 4:  #MOVE_RIGHT\n",
    "            return x + step_size * np.sin(angle_rad), y - step_size * np.cos(angle_rad), angle\n",
    "        elif move == 5:  #MOVE_BACKWARD\n",
    "            return x - step_size * np.cos(angle_rad), y - step_size * np.sin(angle_rad), angle\n",
    "        return x, y, angle\n",
    "\n",
    "    #Function to select next move based on pheromones and heuristic\n",
    "    def select_next_move(x, y, angle):\n",
    "        possible_moves = [0, 1, 2, 3, 4, 5]  #TURN_LEFT, TURN_RIGHT, etc.\n",
    "        probs = []\n",
    "\n",
    "        for move in possible_moves:\n",
    "            #Simulate movement locally without stepping the game\n",
    "            simulated_x, simulated_y, simulated_angle = simulate_move(x, y, angle, move)\n",
    "            print(f\"Simulated move: {move}, x: {simulated_x}, y: {simulated_y}, angle: {simulated_angle}\")  #Debug print\n",
    "\n",
    "            #Convert to grid indices and calculate pheromone + heuristic\n",
    "            grid_x, grid_y = env.coord_to_grid(simulated_x, simulated_y)\n",
    "            if 0 <= grid_x < GRID_WIDTH and 0 <= grid_y < GRID_HEIGHT:\n",
    "                pheromone = pheromone_grid[grid_y, grid_x]\n",
    "                eta = heuristic(grid_x, grid_y)\n",
    "                probs.append((pheromone ** alpha) * (eta ** beta))\n",
    "            else:\n",
    "                probs.append(0.0)  #Invalid moves have zero probability\n",
    "\n",
    "        #Normalize probabilities\n",
    "        probs = np.array(probs)\n",
    "        if probs.sum() == 0:\n",
    "            probs = np.ones(len(possible_moves)) / len(possible_moves)  #Default equal prob\n",
    "        else:\n",
    "            probs /= probs.sum()\n",
    "\n",
    "        return np.random.choice(possible_moves, p=probs)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{iterations}\")\n",
    "        \n",
    "        for ant in range(num_ants):\n",
    "            observation, info = env.reset()\n",
    "            x, y, angle = env.xpos, env.ypos, env.angle\n",
    "            path = []\n",
    "            success = False\n",
    "\n",
    "            for step_count in range(100):  #Limit steps per ant\n",
    "                action = select_next_move(x, y, angle)\n",
    "                observation, _, terminated, _, info = env.step(action)\n",
    "                x, y, angle = info['xpos'], info['ypos'], info['angle']\n",
    "                path.append((env.coord_to_grid(x, y)))\n",
    "                \n",
    "                if (x, y) == GOAL_POSITION:\n",
    "                    success = True\n",
    "                    print(f\"Ant {ant + 1} reached the goal in {step_count} steps.\")\n",
    "                    break\n",
    "                \n",
    "                if terminated:\n",
    "                    break\n",
    "            \n",
    "            #Update pheromones\n",
    "            for (px, py) in path: #Give a little boost to path if the ant reached the goal\n",
    "                pheromone_grid[py, px] += pheromone_deposit if success else 0.0\n",
    "        \n",
    "        #Evaporate pheromones\n",
    "        #pheromone_grid *= (1 - rho) #Right now this is done in env and idk if I want it done in env or here\n",
    "        \n",
    "        print(f\"Pheromone grid updated for iteration {iteration + 1}.\")\n",
    "        plot_pheromone_grid(pheromone_grid, iteration + 1)  #Plot the pheromone grid after each iteration\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pheromone_grid(pheromone_grid, iteration):\n",
    "    plt.imshow(pheromone_grid, cmap='hot', interpolation='nearest')\n",
    "    plt.title(f'Pheromone Grid at Iteration {iteration}')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "#Initialize the environment and run ACO training\n",
    "env = mywayhome_VZG(config_path=doomfinder(\"my_way_home.cfg\"), render=True)\n",
    "aco_training(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions that create the directories that the logs will be saved in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_directory(base_dir=\"runs/vizdoom_ga_defend_the_center\"):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    existing_runs = [int(d.split('_')[-1]) for d in os.listdir(base_dir) if d.split('_')[-1].isdigit()]\n",
    "    run_number = max(existing_runs, default=0) + 1\n",
    "    run_dir = os.path.join(base_dir, f\"run_{run_number}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    #Create log directory within the run directory\n",
    "    log_dir = os.path.join(run_dir, \"log\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    return run_dir, log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define best agents loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_agents(model_dir, num_agents=5):\n",
    "    \"\"\"Load a specified number of best agents from a previous run's model directory.\"\"\"\n",
    "    best_agents = []\n",
    "    for i in range(num_agents):\n",
    "        agent = DoomAgent()\n",
    "        checkpoint_path = os.path.join(model_dir, f\"best_agent_gen_{i}.pth\")\n",
    "        agent.load_state_dict(torch.load(checkpoint_path, weights_only=False)) \n",
    "        best_agents.append(agent)\n",
    "    return best_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with the ViZDoom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Doom environment\n",
    "env = Defend_the_Center_VZG(doomfinder('defend_the_center_modified.cfg'), render=False)\n",
    "\n",
    "env_checker.check_env(env) #Check the environment to see if its valid\n",
    "\n",
    "#Run Genetic Algorithm\n",
    "trained_agents = run_ga(env, generations=1000, pop_size=30, num_parents=5, mutation_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with the ViZDoom enviornment based off a previous best agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Doom environment\n",
    "#env = Defend_the_Center_VZG(doomfinder('defend_the_center_modified.cfg'), render=False)\n",
    "\n",
    "#initial_population = load_best_agents(\"runs/vizdoom_ga_defend_the_center/run_7/saved_models\", num_agents=999)\n",
    "\n",
    "#Run Genetic Algorithm\n",
    "#trained_agents = run_ga(env, generations=1000, pop_size=30, num_parents=5, mutation_rate=0.01, initial_population=initial_population)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test best agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the agent\n",
    "agent = DoomAgent()\n",
    "\n",
    "#Load the saved model weights into the agent\n",
    "agent.load_state_dict(torch.load(\"runs/vizdoom_ga_defend_the_center/run_7/saved_models/best_agent_gen_999.pth\"))\n",
    "\n",
    "#Set the agent to evaluation mode\n",
    "agent.eval()\n",
    "\n",
    "#Initialize the environment\n",
    "env = Defend_the_Center_VZG(doomfinder('defend_the_center.cfg'), render=True)\n",
    "\n",
    "for episode in range(5):\n",
    "    observation, _ = env.reset()  #Reset the environment and get only the observation\n",
    "    done = False  #Set done to false\n",
    "    total_reward = 0  #Set total reward to 0\n",
    "\n",
    "    while not done:  #While the game isn't done\n",
    "        #Convert the observation to a tensor and pass it through the agent\n",
    "        obs_tensor = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            action_probs = agent(obs_tensor)  # Use agent instead of best_agent\n",
    "            action = torch.argmax(action_probs).item()\n",
    "        \n",
    "        #Take a step in the environment\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward  #Add the reward to the total reward\n",
    "        time.sleep(0.05)  #Sleep for 0.05 seconds\n",
    "\n",
    "    print(f'Episode: {episode}, Total Reward: {total_reward}')  #Print the episode and total reward\n",
    "    time.sleep(2)  #Sleep for 2 seconds between episodes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
